<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.5.1"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Reinforcement Learning: An Introduction - Chapter 1 Introduction" /><meta name="author" content="Jayitha C" /><meta property="og:locale" content="en_US" /><meta name="description" content="This is the first in a series of posts which partly summarize chapters from the following book" /><meta property="og:description" content="This is the first in a series of posts which partly summarize chapters from the following book" /><link rel="canonical" href="https://jayitha.github.io/posts/reinforcement-learning-an-introduction-chapter-1-introduction/" /><meta property="og:url" content="https://jayitha.github.io/posts/reinforcement-learning-an-introduction-chapter-1-introduction/" /><meta property="og:site_name" content="Jayitha Cherapanamjeri" /><meta property="og:image" content="https://jayitha.github.io/assets/img/RL_system.jpg" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-01-19T09:03:00+05:30" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://jayitha.github.io/assets/img/RL_system.jpg" /><meta property="twitter:title" content="Reinforcement Learning: An Introduction - Chapter 1 Introduction" /><meta name="twitter:site" content="@jayitha4" /><meta name="twitter:creator" content="@Jayitha C" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Jayitha C"},"description":"This is the first in a series of posts which partly summarize chapters from the following book","image":"https://jayitha.github.io/assets/img/RL_system.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://jayitha.github.io/posts/reinforcement-learning-an-introduction-chapter-1-introduction/"},"@type":"BlogPosting","url":"https://jayitha.github.io/posts/reinforcement-learning-an-introduction-chapter-1-introduction/","headline":"Reinforcement Learning: An Introduction - Chapter 1 Introduction","dateModified":"2021-01-19T09:03:00+05:30","datePublished":"2021-01-19T09:03:00+05:30","@context":"https://schema.org"}</script><title>Reinforcement Learning: An Introduction - Chapter 1 Introduction | Jayitha Cherapanamjeri</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/post.min.js"></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/cal_4.jpeg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Jayitha Cherapanamjeri</a></div><div class="site-subtitle font-italic">A Data Systems Enthusiast.</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <i class="mode-toggle fas fa-sun" dark-mode-invisible></i> <i class="mode-toggle fas fa-moon" light-mode-invisible></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href="https://github.com/Jayitha" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/jayitha4" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jayitha.c','research.iiit.ac.in'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Reinforcement Learning: An Introduction - Chapter 1 Introduction</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Reinforcement Learning: An Introduction - Chapter 1 Introduction</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Jan 19, 2021, 9:03 AM +0530" > Jan 19 <i class="unloaded">2021-01-19T09:03:00+05:30</i> </span> by <span class="author"> Jayitha C </span></div></div><div class="post-content"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/RL_system.jpg" class="post-preview-img"><p>This is the first in a <a href="https://jayitha.github.io/categories/reinforcement-learning-an-introduction/">series</a> of posts which partly summarize chapters from the following book</p><blockquote><p>“Reinforcement Learning: An Introduction” Second Edition, by Richard S. Sutton and Andrew G. Barto.</p></blockquote><p>These posts might be updated later as I understand more.</p><hr /><h2 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h2><p>It is learning what to do such that a numerical reward is maximized. It learns the mapping from situation to action by performing a trial-and-error search. Many times actions taken might not <em>just</em> affect the immediate reward but might also affect subsequent rewards (something akin to local and global optimum).</p><blockquote><p>Both trial-and-error search and delayed reward characterize <em>reinforcement learning (RL)</em></p></blockquote><p>Reinforcement learning is different from</p><ul><li><p><em>supervised learning</em> - In interactive problems (which are what learning agents are more often present in), it might not be possible to get labeled data allowing the agent to learn which the right actions should be. The agent should be able to learn from it’s own experience.</p><li><p><em>unsupervised learning</em> - While reinforcement learning doesn’t rely on labelled “correct” data, it tries to learn situation-action mapping that maximizes a reward rather than just trying to find structure in unlabelled data (which is what the unsupervised paradigm tries to do)</p></ul><p>Reinforcement learning differs from the other two in another way, it suffers from the <strong>exploration-exploitation</strong> dilemma</p><div class="table-wrapper"><table><thead><tr><th style="text-align: left">exploration-exploitation dilemma<tbody><tr><td style="text-align: left">To maximize rewards, the learning agent must play actions that it’s tried before and knows maximizes the expected reward. But to discover maximizing actions, it has to try actions it hasn’t selected before. With stochastic tasks, it has to execute a given action multiple times to be able to estimate the expected reward. This dilemma still remains unsolved.</table></div>\[\text{MACHINE LEARNING} = \begin{cases} \text{SUPERVISED LEARNING}\\\\ \text{UNSUPERVISED LEARNING}\\\\ \text{REINFORCEMENT LEARNING} \end{cases}\]<h3 id="the-paradigm-shift">The Paradigm Shift</h3><ul><li>Since the 1960’s, AI researchers believed that there were no <em>general principles</em> and that intelligence was possessed due to a large number of tricks and heuristics.</ul><blockquote><p>If you could get <em>enough</em> facts in a machine, it would be intelligent.</p></blockquote><ul><li>But in fact too little work had been done to find these <em>general principles</em> and reinforcement learning is trying to find simpler and fewer principles of AI</ul><h2 id="examples">Examples</h2><ul><li><p>A chess game agent. The agent should make moves based on both the current position, possible plays (actions) and the possible counter plays.</p><li><p>Roomba: An automatic vacuum cleaner which enters a room looking for dirt to clean. It has to make a choice - to move into the room or return to a charging port based on how much charge it has and how far it is from the charging port.</p><li><p>A normal person preparing their breakfast.</p></ul><p>And so on …</p><p>All examples stated so far involve a decision-making learning agent <em>interacting</em> with it’s environment to achieve a goal despite any <em>uncertainty</em> it might face (from the environment). Sometimes the agent might not be able to fully predict rewards for a particular action. But the agent will still be able to “see” how close it is to it’s <em>goal</em>. While initial knowledge influences what is important to learn, interaction with the environment is essential for adjusting behavior to exploit features of the task at hand.</p><h2 id="elements-of-reinforcement-learning">Elements of Reinforcement Learning</h2><div class="table-wrapper"><table><thead><tr><th style="text-align: left">Element<th style="text-align: left">Description<tbody><tr><td style="text-align: left"><strong><em>POLICY</em></strong><td style="text-align: left">It is a mapping between perceived states of the environment and actions to be taken when in those states. The policy could be a simple function like a look-up table, or a computationally expensive search process or even stochastic.<tr><td style="text-align: left"><strong><em>REWARD SIGNAL</em></strong><td style="text-align: left">It defines the goal. Every time the agent performs an action, the environment returns a <em>reward</em> which the agent uses to decide if the performed actions are “good” or “bad”. It can be a stochastic function of environment state and actions taken. For example, in humans, rewards can be pleasure or pain. The agent tries to maximize the cumulative reward.<tr><td style="text-align: left"><strong><em>VALUE FUNCTION</em></strong><td style="text-align: left">The <em>value</em> of a state is the total amount of reward an agent expects to accumulate starting from that state. While <em>rewards</em> indicate immediate desirability of a state, <em>value</em> indicate <em>long-term</em> desirability of that state. While we make decisions based the <em>value</em>, <em>values</em> cannot be computed without <em>rewards</em>. <em>value estimation</em> is arguably the most difficult problem within RL<tr><td style="text-align: left"><strong><em>MODEL</em></strong><td style="text-align: left">(optional) mimics the environment allowing inferences to be made about how the environment will behave. Knowing the model allows <em>model-based learning</em> by which a course of action can be decided by considering possible situations <em>before they are experiences</em>. It’s also possible for systems to first use <em>model-free learning</em> (trial-and-error) to learn a model and then <em>plan</em> using this model.</table></div><blockquote><p>Modern RL spans the spectrum - low-level trial-and-error \( \rightarrow \) high-level deliberative planning</p></blockquote><h2 id="limitations-and-scope">Limitations and Scope</h2><ul><li><p>RL relies highly on the <em>state</em> of the environment. As input to policy, value function and model and output from the model.</p><li><p>In the book, they focus mostly on the problem of value estimation. Although <em>value estimation</em> is not necessary - for instance, in evolutionary algorithms like genetic algorithms, genetic programming, simulated annealing and so on policies are learnt through static processes. In cases where there’s a small search space or a long amount of time, evolutionary algorithms might be useful. That being said evolutionary algorithms do not interact with the environment and hence cannot make good use of the structure and information and agent might have.</p></ul><h2 id="example-tic-tac-toe">Example: Tic Tac Toe</h2><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/tic_tac_toe.png" alt="" width="400" /> <em>Tic Tac Toe game</em></p><p>Under the assumption that we are playing an imperfect player (can make “incorrect” moves). Our goal is to construct a player who learns imperfections of opponent’s plays and maximize it’s chances of winning. We might consider the following approaches</p><div class="table-wrapper"><table><thead><tr><th style="text-align: left">Approach<th style="text-align: left">Description<tbody><tr><td style="text-align: left">Classical <em>minimax</em> solution from game theory (GT)<td style="text-align: left">A minimax player would never reach a state in which it could lose even if it can win if the opponent makes an “incorrect” play because this approach assumes the opponent plays a particular way.<tr><td style="text-align: left">Dynamic Programming<td style="text-align: left">This method can compute optimal solutions for any opponent but would require the opponent’s action probability distribution. While practically unavailable, this can be estimated.<tr><td style="text-align: left">Evolutionary Algorithm<td style="text-align: left">A genetic algorithm for example, will consider a sample of policies from the policy space, evaluate those policies through experimentation and mutate those with high rewards. But these methods reward the “whole policy”, even moves that may not have been played.</table></div><p>RL Approach:</p><p>If our agent is playing \(\mathbf{X}s\)</p><ul><li><p>Create a table for each state of the game. For the states where there exists a row of \( 3 \mathbf{X}s \) we set the <em>value</em> to be \( 1 \), the states where there exists a row of \( 3 \mathbf{O}s \) we set the <em>value</em> to be \( 0 \) and \( 0.5 \) for the remaining states where the <em>value</em> indicates the probability of winning from that state.</p><li><p>This agent plays games against the opponent. Out of the possible set of actions the agent <em>greedily</em> picks actions which lead to higher value states. Occasionally random <em>exploratory</em> actions are selected to gain experience. After each <em>greedy</em> move we update the previous state’s value to be <em>closer</em> to the new state.</p></ul><p>For example, if \( S_t \) denotes state at time \( t \), \( V(S_t) \) denotes <em>value</em> of state \( S_t \) and \( S_{t+1} \) denotes state after choosing the <em>greedy</em> action, then \( V(S_t) \) could be updated as follows</p>\[V(S_t) \leftarrow V(S_t) + \alpha \left [ V(S_{t+1} - V(S) \right ]\]<p>where \(\alpha\) is the <em>step-size</em> and this is an example of <em>temporal difference learning</em></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/reinforcement-learning-an-introduction/'>Reinforcement Learning An Introduction</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/textbook/" class="post-tag no-text-decoration" >textbook</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Reinforcement Learning: An Introduction - Chapter 1 Introduction - Jayitha Cherapanamjeri&url=https://jayitha.github.io/posts/reinforcement-learning-an-introduction-chapter-1-introduction/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Reinforcement Learning: An Introduction - Chapter 1 Introduction - Jayitha Cherapanamjeri&u=https://jayitha.github.io/posts/reinforcement-learning-an-introduction-chapter-1-introduction/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Reinforcement Learning: An Introduction - Chapter 1 Introduction - Jayitha Cherapanamjeri&url=https://jayitha.github.io/posts/reinforcement-learning-an-introduction-chapter-1-introduction/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/stuff-i-want-to-read/">Stuff I Want To Read</a><li><a href="/posts/cleaning-uncertain-data-with-quality-guarantees/">Cleaning Uncertain Data with Quality Guarantees</a><li><a href="/posts/text-and-typography/">Text and Typography</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/data-cleaning/">data cleaning</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/textbook/">textbook</a> <a class="post-tag" href="/tags/typography/">typography</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/stuff-i-want-to-read/"><div class="card-body"> <span class="timeago small" > Jan 20 <i class="unloaded">2021-01-20T09:03:00+05:30</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Stuff I Want To Read</h3><div class="text-muted small"><p> I just needed a place to put down all the papers I wanted to summarize and books I want to read \[\newcommand{\E}{\mathop{\mathbb{E}}}\] Papers Paper Status ...</p></div></div></a></div><div class="card"> <a href="/posts/conditional-functional-dependencies-for-data-cleaning/"><div class="card-body"> <span class="timeago small" > Nov 16, 2020 <i class="unloaded">2020-11-16T17:03:00+05:30</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Conditional Functional Dependencies for Data Cleaning</h3><div class="text-muted small"><p> Bohannon, Philip, et al. “Conditional functional dependencies for data cleaning.” 2007 IEEE 23rd international conference on data engineering. IEEE, 2007.</p></div></div></a></div><div class="card"> <a href="/posts/cleaning-uncertain-data-with-quality-guarantees/"><div class="card-body"> <span class="timeago small" > Nov 16, 2020 <i class="unloaded">2020-11-16T17:03:00+05:30</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Cleaning Uncertain Data with Quality Guarantees</h3><div class="text-muted small"><p> Cheng, Reynold, Jinchuan Chen, and Xike Xie. “Cleaning uncertain data with quality guarantees.” Proceedings of the VLDB Endowment 1.1 (2008): 722-735.</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/conditional-functional-dependencies-for-data-cleaning/" class="btn btn-outline-primary"><p>Conditional Functional Dependencies for Data Cleaning</p></a> <a href="/posts/stuff-i-want-to-read/" class="btn btn-outline-primary"><p>Stuff I Want To Read</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://twitter.com/jayitha4">Jayitha</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/data-cleaning/">data cleaning</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/textbook/">textbook</a> <a class="post-tag" href="/tags/typography/">typography</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jayitha.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); </script>
